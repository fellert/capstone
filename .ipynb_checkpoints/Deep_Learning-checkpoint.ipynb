{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional, BatchNormalization, GlobalAveragePooling1D, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, auc\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_non_alphanum, strip_short, strip_numeric\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reduced_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(y_true,y_pred):\n",
    "\n",
    "    y_pred = LabelBinarizer().fit_transform(y_pred.argmax(axis=1))\n",
    "    return roc_auc_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = df['article_text'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "text = article_text.apply(lambda x: strip_short(remove_stopwords(strip_numeric(\n",
    "                            strip_non_alphanum(x.lower()))), minsize=3))\n",
    "\n",
    "title = df.article_title.apply(lambda x: strip_short(remove_stopwords(strip_numeric(\n",
    "                            strip_non_alphanum(x.lower()))), minsize=3))\n",
    "\n",
    "mid_5th = text.apply(lambda x: x[round(0.4*len(x)):round(0.6*len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type = mid_5th\n",
    "max_features = 15000\n",
    "\n",
    "tokenize = Tokenizer(num_words=max_features)\n",
    "tokenize.fit_on_texts(input_type)\n",
    "sequences = tokenize.texts_to_sequences(input_type)\n",
    "max_len = round(float(np.median([len(x) for x in sequences])))\n",
    "input_text = pad_sequences(sequences, maxlen=max_len)\n",
    "labels = to_categorical(df.norm_score, num_classes=3)\n",
    "\n",
    "print(input_text.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "print(\"Median squence length: {}\".format(np.median([len(x) for x in sequences])))\n",
    "print(\"Sequence stdev: {}\".format(np.std([len(x) for x in sequences])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open('glove.6B/glove.6B.100d.txt') as f:  \n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        embeddings_index[word] = np.asarray(line[1:], dtype='float')\n",
    "\n",
    "w_index = tokenize.word_index\n",
    "embedding_dim = 100\n",
    "w_matrix = np.zeros((max_features,embedding_dim))\n",
    "\n",
    "for word, i in w_index.items():\n",
    "    if i < max_features:\n",
    "        glove_vector = embeddings_index.get(word)\n",
    "        if glove_vector is not None:\n",
    "            w_matrix[i] = glove_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_text, labels, test_size=0.1, random_state=1)\n",
    "X_train_rs, y_train_rs = ADASYN(n_jobs=4, random_state=42).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_rs.shape)\n",
    "print(y_train_rs.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(X_train_rs.shape[0])\n",
    "\n",
    "X_train = X_train_rs[idx]\n",
    "y_train = y_train_rs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(X_test.shape[0])\n",
    "\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGULAR LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100, input_length=max_len))\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.layers[0].set_weights([w_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "plt.plot(model.history.history['acc'], color='orange', label=\"Train acc.\")\n",
    "plt.plot(model.history.history['val_acc'], color='lightblue', label=\"Val acc.\")\n",
    "plt.plot(model.history.history['loss'], '--', color='orange', label=\"Train loss\")\n",
    "plt.plot(model.history.history['val_loss'], '--', color=\"lightblue\", label=\"Val loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Train/Test Loss and Accuracy (LSTM)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model.predict(X_test)\n",
    "print(\"AUC: {}\".format(get_auc(y_test,lstm_pred)))\n",
    "\n",
    "confusion_matrix(y_test.argmax(axis=1),model.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn = Sequential()\n",
    "model_lstm_cnn.add(Embedding(max_features, 100, input_length=max_len))\n",
    "model_lstm_cnn.add(Conv1D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_lstm_cnn.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "model_lstm_cnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_lstm_cnn.layers[0].set_weights([w_matrix])\n",
    "model_lstm_cnn.layers[0].trainable = False\n",
    "\n",
    "model_lstm_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_lstm_cnn.summary()\n",
    "\n",
    "model_lstm_cnn.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "plt.plot(model_lstm_cnn.history.history['acc'], color='orange', label=\"Train acc.\")\n",
    "plt.plot(model_lstm_cnn.history.history['val_acc'], color='lightblue', label=\"Val acc.\")\n",
    "plt.plot(model_lstm_cnn.history.history['loss'], '--', color='orange', label=\"Train loss\")\n",
    "plt.plot(model_lstm_cnn.history.history['val_loss'], '--', color=\"lightblue\", label=\"Val loss\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_cnn.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn_pred = model_lstm_cnn.predict(X_test)\n",
    "print(\"AUC: {}\".format(get_auc(y_test,lstm_cnn_pred)))\n",
    "\n",
    "confusion_matrix(y_test.argmax(axis=1),model_lstm_cnn.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUST CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(max_features, 100, input_length=max_len))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling1D(2))\n",
    "model_cnn.add(Conv1D(64, 5, activation='relu'))\n",
    "model_cnn.add(BatchNormalization())\n",
    "model_cnn.add(MaxPooling1D(2))\n",
    "model_cnn.add(Conv1D(64, 5, activation='relu'))\n",
    "model_cnn.add(GlobalAveragePooling1D())\n",
    "model_cnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model_cnn.layers[0].set_weights([w_matrix])\n",
    "model_cnn.layers[0].trainable = False\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_cnn.summary()\n",
    "\n",
    "model_cnn.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "plt.plot(model_cnn.history.history['acc'], color='orange', label=\"Train acc.\")\n",
    "plt.plot(model_cnn.history.history['val_acc'], color='lightblue', label=\"Val acc.\")\n",
    "plt.plot(model_cnn.history.history['loss'], '--', color='orange', label=\"Train loss\")\n",
    "plt.plot(model_cnn.history.history['val_loss'], '--', color=\"lightblue\", label=\"Val loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Train/Val Loss and Accuracy (CNN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.evaluate(X_test, y_test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: {}\".format(get_auc(y_test,model_cnn.predict(X_test))))\n",
    "print(\"Test Accuracy: {}\".format(model_cnn.evaluate(X_test, y_test, verbose=0)[1]))\n",
    "\n",
    "confusion_matrix(y_test.argmax(axis=1),model_cnn.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv('text_and_images.csv')\n",
    "len(df_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TEXT \n",
    "max_features = 2000\n",
    "\n",
    "article_text = df_images['article_text'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "text = article_text.apply(lambda x: strip_short(remove_stopwords(strip_numeric(\n",
    "                            strip_non_alphanum(x.lower()))), minsize=3))\n",
    "\n",
    "tokenize = Tokenizer(num_words=max_features)\n",
    "tokenize.fit_on_texts(text)\n",
    "sequences = tokenize.texts_to_sequences(text)\n",
    "max_len = round(float(np.median([len(x) for x in sequences])))\n",
    "input_text = pad_sequences(sequences, maxlen=max_len)\n",
    "y_binary = to_categorical(df_images.norm_score, num_classes=3)\n",
    "\n",
    "####### IMAGES\n",
    "images_data = df_images.drop(['article_title', 'article_text', 'url', 'norm_score'], axis=1)\n",
    "\n",
    "print(input_text.shape)\n",
    "print(y_binary.shape)\n",
    "print(\"Mean squence length: {}\".format(np.median([len(x) for x in sequences])))\n",
    "print(\"Sequence stdev: {}\".format(np.std([len(x) for x in sequences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_index = tokenize.word_index\n",
    "embedding_dim = 100\n",
    "w_matrix = np.zeros((max_features,embedding_dim))\n",
    "\n",
    "for word, i in w_index.items():\n",
    "    if i < max_features:\n",
    "        glove_vector = embeddings_index.get(word)\n",
    "        if glove_vector is not None:\n",
    "            w_matrix[i] = glove_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(input_text)\n",
    "text_idx = text_df.shape[1]\n",
    "combined = images_data.join(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combined, y_binary, test_size=0.1, random_state=42)\n",
    "X_train_rs, y_train_rs = ADASYN(n_jobs=4, random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "idx = np.random.permutation(X_train_rs.shape[0])\n",
    "\n",
    "X_train = X_train_rs[idx]\n",
    "y_train = y_train_rs[idx]\n",
    "\n",
    "text_features_train = X_train[:,-text_idx:]\n",
    "image_features_train = X_train[:,:-text_idx]\n",
    "\n",
    "text_features_test = X_test[list(text_df.columns)]\n",
    "image_features_test = X_test.drop(list(text_df.columns), axis=1)\n",
    "\n",
    "print(y_train_rs.sum(axis=0))\n",
    "print(y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Text First (reusing from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100, input_length=max_len))\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.layers[0].set_weights([w_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(text_features_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "plt.plot(model.history.history['acc'], color='orange', label=\"Train acc.\")\n",
    "plt.plot(model.history.history['val_acc'], color='lightblue', label=\"Val acc.\")\n",
    "plt.plot(model.history.history['loss'], '--', color='orange', label=\"Train loss\")\n",
    "plt.plot(model.history.history['val_loss'], '--', color=\"lightblue\", label=\"Val loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Train/Test Loss and Accuracy (LSTM)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = model.predict(text_features_test)\n",
    "print(model.evaluate(text_features_test, y_test, verbose=0)[1])\n",
    "print(\"AUC: {}\".format(get_auc(y_test,lstm_pred)))\n",
    "\n",
    "confusion_matrix(y_test.argmax(axis=1),model.predict(text_features_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(None,),dtype='int32',name='text')\n",
    "embedding = Embedding(max_features, 100, input_length=max_len)(text_input)\n",
    "lstm = LSTM(100, dropout=0.1, recurrent_dropout=0.1)(embedding)\n",
    "\n",
    "image_input = Input(shape=(images_data.shape[1],), dtype='float32', name='images')\n",
    "image_network = Dense(128,input_shape=(len(df),), activation='relu')(image_input)\n",
    "image_network = Dropout(0.2)(image_network)\n",
    "image_network = Dense(64,input_shape=(len(df),), activation='relu')(image_network)\n",
    "\n",
    "concatenated = concatenate([lstm, image_network],axis=-1)\n",
    "result = Dense(3, activation='softmax')(concatenated)\n",
    "\n",
    "model_images = Model([text_input,image_input], result)\n",
    "model_images.layers[3].set_weights = w_matrix\n",
    "model_images.layers[3].trainable = False\n",
    "\n",
    "model_images.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "model_images.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_images.fit([text_features_train, image_features_train], y_train, epochs=5, \n",
    "                 batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_images.evaluate([text_features_test,image_features_test], y_test, verbose=0)[1])\n",
    "print(confusion_matrix(y_test.argmax(axis=1),model_images.predict([text_features_test, \n",
    "                                                                   image_features_test]).argmax(axis=1)))\n",
    "\n",
    "print(classification_report(y_test.argmax(axis=1),model_images.predict([text_features_test, \n",
    "                                                                   image_features_test]).argmax(axis=1)))\n",
    "\n",
    "combined_pred = model_images.predict([text_features_test, image_features_test])\n",
    "print(\"AUC: {}\".format(get_auc(y_test,combined_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_images.history.history['acc'], color='orange', label=\"Train acc.\")\n",
    "plt.plot(model_images.history.history['val_acc'], color='lightblue', label=\"Val acc.\")\n",
    "plt.plot(model_images.history.history['loss'], '--', color='orange', label=\"Train loss\")\n",
    "plt.plot(model_images.history.history['val_loss'], '--', color=\"lightblue\", label=\"Val loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Train/Val Loss and Accuracy (Text + Image Features)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
