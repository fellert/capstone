{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional, BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_non_alphanum, strip_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print(len(df))\n",
    "df = df[~df.article_text.str.contains('403 - Forbidden', regex=False)]\n",
    "df = df[~df.article_title.str.contains('404 | Fox News', regex=False)]\n",
    "df = df[~df.article_title.str.contains('Page Not Found', regex=False)]\n",
    "df = df[~df.article_text.str.contains('Page Not Found', regex=False)]\n",
    "df = df[df.article_text.str.len() >= 10]\n",
    "df = df.drop_duplicates(subset='url')\n",
    "df.reset_index()\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {\n",
    "    \"Negative\": -1.0,\n",
    "    \"SomewhatNegative\": -0.5,\n",
    "    \"Neutral\": 0.0,\n",
    "    \"SomewhatPositive\": 0.5,\n",
    "    \"Positive\": 1.0\n",
    "}\n",
    "\n",
    "df['democrat.vote'] = df['democrat.vote'].map(score)\n",
    "df['republican.vote'] = df['republican.vote'].map(score)\n",
    "df['norm_score'] = (df['republican.vote'] - df['democrat.vote']) / 2\n",
    "\n",
    "df.norm_score = df.norm_score.map({\n",
    "                          -1.0: -1.0,\n",
    "                          -0.75: -1.0, \n",
    "                          -0.5: -1.0,\n",
    "                          -0.25: 0.0,\n",
    "                          0.0: 0.0,\n",
    "                          0.25: 0.0,\n",
    "                          0.5: 1.0,\n",
    "                          0.75: 1.0, \n",
    "                          1.0: 1.0\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('outlet')['norm_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.outlet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df.norm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reduced_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv('8imageSpareMatrix.csv')\n",
    "df_combined = pd.read_csv('9articlesAndImageMatrix.csv', dtype={'url' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_images))\n",
    "print(len(df_combined))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.dropna()\n",
    "len(df_combined)\n",
    "\n",
    "collapsed = df_combined.groupby('url').mean()\n",
    "print(len(collapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_text = pd.merge(collapsed, df[['article_text', 'article_title', 'norm_score', 'url']], on='url', how='left')\n",
    "with_text = with_text.dropna()\n",
    "print(len(with_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_text.to_csv('text_and_images.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
